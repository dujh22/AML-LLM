@article{cheng2023black,
  author = {Cheng, Jiale and Liu, Xiao and Zheng, Kehan and Ke, Pei and Wang, Hongning},
  title = {Black-Box Prompt Optimization: Aligning Large Language Models without Model Training},
  journal = {arXiv preprint arXiv:2311.04155},
  year = {2023},
  archiveprefix = {arXiv}
}

@article{ding2021cogview,
  author = {Ding, Ming and Yang, Zhuoyi and Hong, Wenyi and Zheng, Wendi and Zhou, Chang},
  title = {Cogview: Mastering Text-to-Image Generation via Transformers},
  journal = {Advances in neural information processing systems},
  volume = {34},
  pages = {19822--19835},
  year = {2021}
}

@article{ding2022cogview2,
  author = {Ding, Ming and Zheng, Wendi and Hong, Wenyi and Tang, Jie},
  title = {Cogview2: Faster and Better Text-to-Image Generation via Hierarchical Transformers},
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {16890--16902},
  year = {2022}
}

@article{du2021glm,
  author = {Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  title = {Glm: General Language Model Pretraining with Autoregressive Blank Infilling},
  journal = {arXiv preprint arXiv:2103.10360},
  year = {2021},
  archiveprefix = {arXiv}
}

@article{glm2024chatglm,
  author = {GLM, Team and Zeng, Aohan and Xu, Bin and Wang, Bowen and Zhang, Chenhui},
  title = {Chatglm: A Family of Large Language Models from Glm-130b to Glm-4 All Tools},
  journal = {arXiv preprint arXiv:2406.12793},
  year = {2024},
  archiveprefix = {arXiv}
}

@article{hong2022cogvideo,
  author = {Hong, Wenyi and Ding, Ming and Zheng, Wendi and Liu, Xinghan and Tang, Jie},
  title = {Cogvideo: Large-Scale Pretraining for Text-to-Video Generation via Transformers},
  journal = {arXiv preprint arXiv:2205.15868},
  year = {2022},
  archiveprefix = {arXiv}
}

@inproceedings{liu2023webglm,
  author = {Liu, Xiao and Lai, Hanyu and Yu, Hao and Xu, Yifan and Zeng, Aohan},
  title = {WebGLM: Towards an Efficient Web-Enhanced Question Answering System with Human Preferences},
  pages = {4549--4560},
  year = {2023},
  booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}
}

@article{teng2023relay,
  author = {Teng, Jiayan and Zheng, Wendi and Ding, Ming and Hong, Wenyi and Wangni, Jianqiao and Yang, Zhuoyi and Tang, Jie},
  title = {Relay Diffusion: Unifying Diffusion Process across Resolutions for Image Synthesis},
  journal = {arXiv preprint arXiv:2309.03350},
  year = {2023},
  archiveprefix = {arXiv}
}

@article{wang2024cogvlm,
  author = {Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji},
  title = {Cogvlm: Visual Expert for Pretrained Language Models},
  journal = {Advances in Neural Information Processing Systems},
  volume = {37},
  pages = {121475--121499},
  year = {2024}
}

@article{zeng2022glm,
  author = {Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu},
  title = {Glm-130b: An Open Bilingual Pre-Trained Model},
  journal = {arXiv preprint arXiv:2210.02414},
  year = {2022},
  archiveprefix = {arXiv}
}

@article{zeng2023agenttuning,
  author = {Zeng, Aohan and Liu, Mingdao and Lu, Rui and Wang, Bowen and Liu, Xiao and Dong, Yuxiao and Tang, Jie},
  title = {Agenttuning: Enabling Generalized Agent Abilities for Llms},
  journal = {arXiv preprint arXiv:2310.12823},
  year = {2023},
  archiveprefix = {arXiv}
}

@inproceedings{zheng2023codegeex,
  author = {Zheng, Qinkai and Xia, Xiao and Zou, Xu and Dong, Yuxiao and Wang, Shan},
  title = {Codegeex: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on Humaneval-x},
  pages = {5673--5684},
  year = {2023},
  booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}
}
