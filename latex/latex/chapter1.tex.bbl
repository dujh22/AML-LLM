\begin{thebibliography}{12}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Cheng et~al.(2023)Cheng, Liu, Zheng, Ke, and Wang]{cheng2023black}
Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, and Hongning Wang.
\newblock Black-box prompt optimization: Aligning large language models without
  model training.
\newblock \emph{arXiv preprint arXiv:2311.04155}, 2023.

\bibitem[Ding et~al.(2021)Ding, Yang, Hong, Zheng, and Zhou]{ding2021cogview}
Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, and Chang Zhou.
\newblock Cogview: Mastering text-to-image generation via transformers.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 19822--19835, 2021.

\bibitem[Ding et~al.(2022)Ding, Zheng, Hong, and Tang]{ding2022cogview2}
Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.
\newblock Cogview2: Faster and better text-to-image generation via hierarchical
  transformers.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 16890--16902, 2022.

\bibitem[Du et~al.(2021)Du, Qian, Liu, Ding, Qiu, Yang, and Tang]{du2021glm}
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and
  Jie Tang.
\newblock Glm: General language model pretraining with autoregressive blank
  infilling.
\newblock \emph{arXiv preprint arXiv:2103.10360}, 2021.

\bibitem[GLM et~al.(2024)GLM, Zeng, Xu, Wang, and Zhang]{glm2024chatglm}
Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, and Chenhui Zhang.
\newblock Chatglm: A family of large language models from glm-130b to glm-4 all
  tools.
\newblock \emph{arXiv preprint arXiv:2406.12793}, 2024.

\bibitem[Hong et~al.(2022)Hong, Ding, Zheng, Liu, and Tang]{hong2022cogvideo}
Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang.
\newblock Cogvideo: Large-scale pretraining for text-to-video generation via
  transformers.
\newblock \emph{arXiv preprint arXiv:2205.15868}, 2022.

\bibitem[Liu et~al.(2023)Liu, Lai, Yu, Xu, and Zeng]{liu2023webglm}
Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, and Aohan Zeng.
\newblock Webglm: Towards an efficient web-enhanced question answering system
  with human preferences.
\newblock In \emph{Proceedings of the 29th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining}, pages 4549--4560, 2023.

\bibitem[Teng et~al.(2023)Teng, Zheng, Ding, Hong, Wangni, Yang, and
  Tang]{teng2023relay}
Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang,
  and Jie Tang.
\newblock Relay diffusion: Unifying diffusion process across resolutions for
  image synthesis.
\newblock \emph{arXiv preprint arXiv:2309.03350}, 2023.

\bibitem[Wang et~al.(2024)Wang, Lv, Yu, Hong, and Qi]{wang2024cogvlm}
Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, and Ji~Qi.
\newblock Cogvlm: Visual expert for pretrained language models.
\newblock \emph{Advances in Neural Information Processing Systems},
  37:\penalty0 121475--121499, 2024.

\bibitem[Zeng et~al.(2022)Zeng, Liu, Du, Wang, and Lai]{zeng2022glm}
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, and Hanyu Lai.
\newblock Glm-130b: An open bilingual pre-trained model.
\newblock \emph{arXiv preprint arXiv:2210.02414}, 2022.

\bibitem[Zeng et~al.(2023)Zeng, Liu, Lu, Wang, Liu, Dong, and
  Tang]{zeng2023agenttuning}
Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie
  Tang.
\newblock Agenttuning: Enabling generalized agent abilities for llms.
\newblock \emph{arXiv preprint arXiv:2310.12823}, 2023.

\bibitem[Zheng et~al.(2023)Zheng, Xia, Zou, Dong, and Wang]{zheng2023codegeex}
Qinkai Zheng, Xiao Xia, Xu~Zou, Yuxiao Dong, and Shan Wang.
\newblock Codegeex: A pre-trained model for code generation with multilingual
  benchmarking on humaneval-x.
\newblock In \emph{Proceedings of the 29th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining}, pages 5673--5684, 2023.

\end{thebibliography}
